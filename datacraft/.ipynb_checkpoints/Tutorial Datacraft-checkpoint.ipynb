{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6240bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Levenshtein import distance as lev\n",
    "import nltk\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(),'../'))\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16bf4582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: POT in /Users/colombo/opt/anaconda3/lib/python3.9/site-packages (0.8.1.0)\n",
      "Collecting geomloss\n",
      "  Downloading geomloss-0.2.4-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/colombo/opt/anaconda3/lib/python3.9/site-packages (from POT) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/colombo/opt/anaconda3/lib/python3.9/site-packages (from POT) (1.20.3)\n",
      "Installing collected packages: geomloss\n",
      "Successfully installed geomloss-0.2.4\n"
     ]
    }
   ],
   "source": [
    "! pip install POT geomloss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4d7940",
   "metadata": {},
   "source": [
    "# Evaluation of NLG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8901cfad",
   "metadata": {},
   "source": [
    "Let's start with the first part of the talk and play with the three mentionned types of metrics:\n",
    "\n",
    "- Edit based\n",
    "- N-gram based\n",
    "- Embedding based\n",
    "- Beyoud embedding based metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186a1f66",
   "metadata": {},
   "source": [
    "## Example of Edit based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29de1fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'il fait beau dehors'\n",
    "s2 = \"le temps est magnifique\"\n",
    "lev(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20e265c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'il fait moche dehors'\n",
    "s2 = \"j'aime les gateaux\"\n",
    "lev(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555ea927",
   "metadata": {},
   "source": [
    "## Example of N-gram based Metrics : BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6231eac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4548019047027907"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
    "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
    "nltk.translate.bleu_score.sentence_bleu([reference], hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d1b147",
   "metadata": {},
   "source": [
    "## Example of Embedding Based Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0811fa14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = [\n",
    "        'I like my cakes very much',\n",
    "        'I hate these cakes!']\n",
    "hypothesis = ['I like my cakes very much',\n",
    "                  'I like my cakes very much']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1afbe7",
   "metadata": {},
   "source": [
    "You should clone https://github.com/PierreColombo/nlg_eval_via_simi_measures/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21736d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clonage dans 'nlg_eval_via_simi_measures'...\n",
      "remote: Enumerating objects: 281, done.\u001b[K\n",
      "remote: Counting objects: 100% (281/281), done.\u001b[K\n",
      "remote: Compressing objects: 100% (212/212), done.\u001b[K\n",
      "lta 169), reused 140 (delta 66), pack-reused 0\u001b[K65 Mio/s7.36 Mio | 3.65 Mio/s\n",
      "Réception d'objets: 100% (281/281), 8.78 Mio | 3.58 Mio/s, fait.\n",
      "Résolution des deltas: 100% (169/169), fait.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/PierreColombo/nlg_eval_via_simi_measures/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb50206",
   "metadata": {},
   "source": [
    "### Single Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5ad81aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlg_eval_via_simi_measures.depth_score import DepthScoreMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f037d51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596a22b1c5764ca892fe4ada2c0f8b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d913e2d974e3444a9350640376f8b12b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7acd51d801e64e73afcc5fe161e8c730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ccb69e35744e7a435bef1e3327e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4637080460e457e9964fe88a76654db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Depth Score Progress: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'depth_score': [0.0, 0.11104687618997966]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_call = DepthScoreMetric()\n",
    "metric_call.prepare_idfs(ref, hypothesis)\n",
    "final_preds = metric_call.evaluate_batch(ref, hypothesis)\n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178fa6c",
   "metadata": {},
   "source": [
    "### Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a52b10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlg_eval_via_simi_measures.bary_score import BaryScoreMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10d2f4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "BaryScore Progress:   0%|                                                                                                                                                                       | 0/2 [00:00<?, ?it/s]/Users/colombo/opt/anaconda3/lib/python3.9/site-packages/ot/bregman.py:517: UserWarning: Sinkhorn did not converge. You might want to increase the number of iterations `numItermax` or the regularization parameter `reg`.\n",
      "  warnings.warn(\"Sinkhorn did not converge. You might want to \"\n",
      "BaryScore Progress:  50%|███████████████████████████████████████████████████████████████████████████████▌                                                                               | 1/2 [00:00<00:00,  3.78it/s]/Users/colombo/opt/anaconda3/lib/python3.9/site-packages/ot/bregman.py:484: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  v = b / KtransposeU\n",
      "/Users/colombo/opt/anaconda3/lib/python3.9/site-packages/ot/bregman.py:492: UserWarning: Warning: numerical errors at iteration 0\n",
      "  warnings.warn('Warning: numerical errors at iteration %d' % ii)\n",
      "BaryScore Progress: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'baryscore_W': [1.3877787807814457e-16, 0.4936737905316958], 'baryscore_SD_10': [0.9234964428954497, 1.0454139527060042], 'baryscore_SD_1': [0.7360736875507013, 0.9437505231734742], 'baryscore_SD_5': [0.9074754382695642, 1.0362072097903658], 'baryscore_SD_0.1': [0.0007089172440395627, 0.5100520682416732], 'baryscore_SD_0.5': [0.46239889572851234, 0.8098912034871392], 'baryscore_SD_0.01': [1.3877787807814496e-16, 0.4936737913516362], 'baryscore_SD_0.001': [1.3877787807814457e-16, 3.118899427590597e-08]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_call = BaryScoreMetric()\n",
    "metric_call.prepare_idfs(ref, hypothesis)\n",
    "final_preds = metric_call.evaluate_batch(ref, hypothesis)\n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f05fd21",
   "metadata": {},
   "source": [
    "## Beyond Embedding Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d56b471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlg_eval_via_simi_measures.infolm import InfoLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81a1e525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fisher_rao': [0.0, 1.5417792797088623], 'r_fisher_rao': [0.0, 1.5417792797088623], 'sim_fisher_rao': [0.0, 1.5417792797088623]}\n"
     ]
    }
   ],
   "source": [
    "metric_call = InfoLM(measure_to_use='fisher_rao', alpha=0.25, beta=0.25, temperature=1, use_idf_weights=False)\n",
    "metric_call.prepare_idfs(ref, hypothesis)\n",
    "final_preds = metric_call.evaluate_batch(ref, hypothesis)\n",
    "print(final_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7b07b0",
   "metadata": {},
   "source": [
    "# Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88740fa5",
   "metadata": {},
   "source": [
    "## Getting the data for scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065112f3",
   "metadata": {},
   "source": [
    "$$Score_1 = \\begin{bmatrix} &  T_{1} & \\cdots & T_{|T|}\\\\\n",
    "S_1 & \\cdots & \\cdots & \\cdots \\\\\n",
    "S_2 & \\cdots &\\cdots & \\cdots \\\\\n",
    "\\cdots &\\cdots &\\cdots& \\cdots \\\\\n",
    "S_{|S|} & \\cdots& \\cdots& \\cdots  \\\\\n",
    "\\end{bmatrix}$$, \n",
    "\n",
    "where each score of the task is a column of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f06d8835",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['xtrem','glue','superglue'] # available datasets\n",
    "number = 0 # which dataset to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72f28cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading xtrem\n",
      "Shape (15, 5)\n",
      "Loading glue\n",
      "Shape (105, 15)\n",
      "Loading superglue\n",
      "Shape (24, 15)\n",
      "Number of points 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/colombo/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for ds in  DATASETS:\n",
    "    print('Loading',ds)\n",
    "    df =  pd.read_csv('../data/{}.data'.format(ds),index_col=False)\n",
    "    df =  pd.read_csv('../data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1]) #filter columns\n",
    "    count +=  df.shape[0]* df.shape[1]\n",
    "    print('Shape',df.shape)\n",
    "print('Number of points',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92e971e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading xtrem\n"
     ]
    }
   ],
   "source": [
    "print('Loading', DATASETS[number])\n",
    "df =  pd.read_csv('../data/{}.data'.format(DATASETS[number]),index_col=False)\n",
    "df =  pd.read_csv('../data/{}.data'.format(DATASETS[number]),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1]) #filter columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d7531d",
   "metadata": {},
   "source": [
    "## Comparing the different rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2d48b",
   "metadata": {},
   "source": [
    "### Mean Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fb2c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking mean [14 13 11 10 12  9  7  6  8  5  3  4  2  0  1]\n",
      "Means [83.2   82.65  80.55  80.275 80.625 79.675 79.175 79.025 79.45  78.6\n",
      " 76.225 78.325 73.5   60.075 61.95 ]\n",
      "ranking_borda_reindex [13, 14, 12, 10, 11, 9, 7, 6, 8, 5, 3, 2, 4, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "mean_ranking,means = mean_aggregation_task_level(df)\n",
    "mean_reindex = [mean_ranking.tolist().index(i) for i in range(len(mean_ranking))]\n",
    "print('Ranking mean',mean_ranking)\n",
    "print('Means',means)\n",
    "print('ranking_borda_reindex',mean_reindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746bd8d",
   "metadata": {},
   "source": [
    "### Borda Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8d17c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking Borda [14 13 12 11  9 10  6  7  8  4  3  5  2  0  1]\n",
      "Count Borda [55 50 44 42 34 35 29 29 31 21 15 22  9  1  3]\n",
      "ranking_borda_reindex [13, 14, 12, 10, 9, 11, 6, 7, 8, 4, 5, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "ranking_borda,borda_count = ranking_aggregation(df,True)\n",
    "ranking_borda_reindex = [ranking_borda.tolist().index(i) for i in range(len(ranking_borda))]\n",
    "print('Ranking Borda',ranking_borda)\n",
    "print('Count Borda',borda_count)\n",
    "print('ranking_borda_reindex',ranking_borda_reindex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
